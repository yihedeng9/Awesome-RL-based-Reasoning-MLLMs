<div align="center">

# Awesome RL-based Reasoning MLLMs

[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

</div>

Recent advancements in leveraging reinforcement learning to enhance LLM reasoning capabilities have yielded remarkably promising results, exemplified by [DeepSeek-R1](https://arxiv.org/pdf/2501.12948), [Kimi k1.5](https://arxiv.org/pdf/2501.12599), [OpenAI o3-mini](https://openai.com/index/o3-mini-system-card/), [Grok 3](https://x.ai/blog/grok-3). These exhilarating achievements herald ascendance of Large Reasoning Models, making us advance further along the thorny path towards Artificial General Intelligence (AGI). Study of LLM reasoning has garnered significant attention within the community, and researchers have concurrently summarized [awesome RL-based LLM reasoning](https://github.com/bruno686/Awesome-RL-based-LLM-Reasoning). Meanwhile, we have observed that remarkably awesome work has already been done in the domain of Multimodal Large Language Models (MLLMs), encompassing both **multimodal understanding** and **autoregressive text-to-image generation**.
<div align="center">
    "The senses are the organs by which man perceives the world, and the soul acts through them as through tools."
</div>
<div align="right">
â€” Leonardo da Vinci
</div>
This repository provides valuable reference for researchers in the field of multimodality, please start your exploratory travel in RL-based Reasoning MLLMs!

## PapersğŸ“„

* [2503] [R1-Omni] [R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/abs/2503.05379) (Alibaba) [Model ğŸ¤—](https://huggingface.co/StarJiaxing/R1-Omni-0.5B) [Code ğŸ’»](https://github.com/HumanMLLM/R1-Omni)

* [2503] [Vision-R1] [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/pdf/2503.06749) (ECNU) [Code ğŸ’»](https://github.com/Osilly/Vision-R1)

* [2503] [Seg-Zero] [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement](https://arxiv.org/pdf/2503.06520) (CUHK) [Model ğŸ¤—](https://huggingface.co/Ricky06662/Seg-Zero-7B) [Dataset ğŸ¤—](https://huggingface.co/datasets/Ricky06662/refCOCOg_2k_840) [Code ğŸ’»](https://github.com/dvlab-research/Seg-Zero)

* [2503] [MM-Eureka] [MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://github.com/ModalMinds/MM-EUREKA/blob/main/MM_Eureka_paper.pdf) (Shanghai AI Laboratory) [Models ğŸ¤—](https://huggingface.co/FanqingM) [Dataset ğŸ¤—](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset) [Code ğŸ’»](https://github.com/ModalMinds/MM-EUREKA)

* [2503] [Visual-RFT] [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/pdf/2503.01785) (SJTU) [Project ğŸŒ](https://github.com/Liuziyu77/Visual-RFT) [Datasets ğŸ¤—](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df) [Code ğŸ’»](https://github.com/Liuziyu77/Visual-RFT)

* [2502] [MedVLM-R1] [MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning](https://arxiv.org/pdf/2502.19634) (TUM)
  
* [2501] [Kimi k1.5] [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/pdf/2501.12599) (MoonshotAI) [Project ğŸŒ](https://github.com/MoonshotAI/Kimi-k1.5)
  
* [2501] [Mulberry] [Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/pdf/2412.18319) (THU) [Model ğŸ¤—](https://huggingface.co/HuanjinYao/Mulberry_llava_8b) [Code ğŸ’»](https://github.com/HJYao00/Mulberry)

* [2501] [Virgo] [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904v2) (RUC) [Model ğŸ¤—](https://huggingface.co/RUC-AIBOX/Virgo-72B) [Code ğŸ’»](https://github.com/RUCAIBox/Virgo)
  
* [2501] [Text-to-image COT] [Can We Generate Images with CoT? Letâ€™s Verify and Reinforce Image Generation Step by Step](https://arxiv.org/pdf/2501.13926) (CUHK) [Project ğŸŒ](https://github.com/ZiyuGuo99/Image-Generation-CoT) [Model ğŸ¤—](https://huggingface.co/ZiyuG/Image-Generation-CoT)  [Code ğŸ’»](https://github.com/ZiyuGuo99/Image-Generation-CoT)
  
* [2501] [LlamaV-o1] [LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs](https://arxiv.org/pdf/2501.06186) (MBZUAI) [Project ğŸŒ](https://mbzuai-oryx.github.io/LlamaV-o1/) [Model ğŸ¤—](https://huggingface.co/omkarthawakar/LlamaV-o1)  [Code ğŸ’»](https://github.com/mbzuai-oryx/LlamaV-o1)

* [2411] [InternVL2-MPO] [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442) (Shanghai AI Laboratory) [Project ğŸŒ](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/) [Model ğŸ¤—](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO) [Code ğŸ’»](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)

* [2411] [LLaVA-CoT] [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440v4) (PKU) [Project ğŸŒ](https://github.com/PKU-YuanGroup/LLaVA-CoT) [Model ğŸ¤—](https://huggingface.co/Xkev/Llama-3.2V-11B-cot) [DemoğŸ¤—](https://huggingface.co/spaces/Xkev/Llama-3.2V-11B-cot) [Code ğŸ’»](https://github.com/PKU-YuanGroup/LLaVA-CoT)


## BenchmarksğŸ“Š

* [2502] [MME-CoT] [MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/abs/2502.09621) (CUHK) [Project ğŸŒ](https://mmecot.github.io/) [Dataset ğŸ¤—](https://huggingface.co/datasets/CaraJ/MME-CoT) [Code ğŸ’»](https://github.com/CaraJ7/MME-CoT)

* [2502] [ZeroBench] [ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models](https://arxiv.org/pdf/2502.09696) (Cambridge) [Project ğŸŒ](https://zerobench.github.io/) [Dataset ğŸ¤—](https://huggingface.co/datasets/jonathan-roberts1/zerobench) [Code ğŸ’»](https://github.com/jonathan-roberts1/zerobench/)


## Open-Source ProjectsğŸŒ

* [EasyR1 ğŸ’»](https://github.com/hiyouga/EasyR1)  ![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1) (An Efficient, Scalable, Multi-Modality RL Training Framework)

* [Multimodal Open R1 ğŸ’»](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal) [Model ğŸ¤—](https://huggingface.co/lmms-lab/Qwen2-VL-2B-GRPO-8k) [Dataset ğŸ¤—](https://huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified)
  
* [LMM-R1 ğŸ’»](https://github.com/TideDra/lmm-r1) ![LMM-R1](https://img.shields.io/github/stars/TideDra/lmm-r1) [Code ğŸ’»](https://github.com/TideDra/lmm-r1)

* [R1-Multimodal-Journey ğŸ’»](https://github.com/FanqingM/R1-Multimodal-Journey) ![R1-Multimodal-Journey](https://img.shields.io/github/stars/FanqingM/R1-Multimodal-Journey) (Latest progress at [MM-Eureka](https://github.com/ModalMinds/MM-EUREKA))

* [R1-V ğŸ’»](https://github.com/Deep-Agent/R1-V)  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V) [Blog ğŸ¯](https://deepagent.notion.site/rlvr-in-vlms) [Datasets ğŸ¤—](https://huggingface.co/collections/MMInstruction/r1-v-67aae24fa56af9d2e2755f82)
  
* [VLM-R1 ğŸ’»](https://github.com/om-ai-lab/VLM-R1)  ![VLM-R1](https://img.shields.io/github/stars/om-ai-lab/VLM-R1) [Model ğŸ¤—](https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps)  [Dataset ğŸ¤—](https://huggingface.co/datasets/omlab/VLM-R1) [Demo ğŸ¤—](https://huggingface.co/spaces/omlab/VLM-R1-Referral-Expression)

* [R1-Vision ğŸ’»](https://github.com/yuyq96/R1-Vision) ![R1-Vision](https://img.shields.io/github/stars/yuyq96/R1-Vision) [Cold-Start Datasets ğŸ¤—](https://huggingface.co/collections/yuyq96/r1-vision-67a6fb7898423dca453efa83)

* [R1-Onevision ğŸ’»](https://github.com/Fancy-MLLM/R1-Onevision)  ![R1-Onevision](https://img.shields.io/github/stars/Fancy-MLLM/R1-Onevision) [Model ğŸ¤—](https://huggingface.co/Fancy-MLLM/R1-Onevision-7B)  [Dataset ğŸ¤—](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision) [Demo ğŸ¤—](https://huggingface.co/spaces/Fancy-MLLM/R1-Onevision) [Report ğŸ“](https://yangyi-vai.notion.site/r1-onevision)

* [VisualThinker-R1-Zero ğŸ’»](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)  ![R1-Onevision](https://img.shields.io/github/stars/turningpoint-ai/VisualThinker-R1-Zero) [Report ğŸ“](https://turningpointai.notion.site/the-multimodal-aha-moment-on-2b-model) (Aha Moment on a 2B non-SFT Model)

* [Open R1 Video ğŸ’»](https://github.com/Wang-Xiaodong1899/Open-R1-Video) ![Open R1 Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video) [Models ğŸ¤—](https://huggingface.co/Xiaodong/Open-R1-Video-7B)  [Datasets ğŸ¤—](https://huggingface.co/datasets/Xiaodong/open-r1-video-4k) [Datasets ğŸ¤—](https://huggingface.co/datasets/Xiaodong/open-r1-video-4k)

* [Video-R1 ğŸ’»](https://github.com/tulerfeng/Video-R1) ![Video-R1](https://img.shields.io/github/stars/tulerfeng/Video-R1) [Code ğŸ’»](https://github.com/tulerfeng/Video-R1)
 [Dataset ğŸ¤—](https://huggingface.co/datasets/Video-R1/DVD-counting)


##  Star Chartâ­

[![Star History Chart](https://api.star-history.com/svg?repos=Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs)](https://star-history.com/#Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs&Date)
