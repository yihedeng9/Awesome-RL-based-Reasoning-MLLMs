<div align="center">

# Awesome RL-based Reasoning MLLMs

[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

</div>

Recent advancements in leveraging reinforcement learning to enhance LLM reasoning capabilities have yielded remarkably promising results, exemplified by [DeepSeek-R1](https://arxiv.org/pdf/2501.12948), [Kimi k1.5](https://arxiv.org/pdf/2501.12599), [OpenAI o3-mini](https://openai.com/index/o3-mini-system-card/), [Grok 3](https://x.ai/blog/grok-3). These exhilarating achievements herald ascendance of Large Reasoning Models, making us advance further along the thorny path towards Artificial General Intelligence (AGI). Study of LLM reasoning has garnered significant attention within the community, and researchers have concurrently summarized [awesome RL-based LLM reasoning](https://github.com/bruno686/Awesome-RL-based-LLM-Reasoning). Meanwhile, we have observed that remarkably awesome work has already been done in the domain of Multimodal Large Language Models (MLLMs), encompassing both **multimodal understanding** and **autoregressive text-to-image generation**.
<div align="center">
    "The senses are the organs by which man perceives the world, and the soul acts through them as through tools."
</div>
<div align="right">
â€” Leonardo da Vinci
</div>
This repository provides valuable reference for researchers in the field of multimodality, please start your exploratory travel in RL-based Reasoning MLLMs!

## Papers
* [2501] [Janus-Pro] [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](http://arxiv.org/pdf/2501.17811) (DeepSeek)
  [ProjectğŸŒ](https://github.com/deepseek-ai/Janus)  [Models ğŸ¤— [7B](https://huggingface.co/deepseek-ai/Janus-Pro-7B) [1B](https://huggingface.co/deepseek-ai/Janus-Pro-1B)]  [Demo ğŸ¤—](https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B)
* [2501] [Kimi k1.5] [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/pdf/2501.12599) (MoonshotAI) [ProjectğŸŒ](https://github.com/MoonshotAI/Kimi-k1.5)
* [2501] [Text-to-image COT] [Can We Generate Images with CoT? Letâ€™s Verify and Reinforce Image Generation Step by Step](https://arxiv.org/pdf/2501.13926) (CUHK) [ProjectğŸŒ](https://github.com/ZiyuGuo99/Image-Generation-CoT) [Model ğŸ¤—](https://huggingface.co/ZiyuG/Image-Generation-CoT)  [Code ğŸ’»](https://github.com/ZiyuGuo99/Image-Generation-CoT)
* [2501] [LlamaV-o1] [LlamaV-o1: Rethinking Step-by-step
Visual Reasoning in LLMs](https://arxiv.org/pdf/2501.06186)(MBZUAI) [ProjectğŸŒ](https://mbzuai-oryx.github.io/LlamaV-o1/) [Model ğŸ¤—](https://huggingface.co/omkarthawakar/LlamaV-o1)  [Code ğŸ’»](https://github.com/mbzuai-oryx/LlamaV-o1)
* [2411] [LLaVA-CoT] [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440v4) (PKU) [ProjectğŸŒ](https://github.com/PKU-YuanGroup/LLaVA-CoT) [Model ğŸ¤—](https://huggingface.co/Xkev/Llama-3.2V-11B-cot) [DemoğŸ¤—](https://huggingface.co/spaces/Xkev/Llama-3.2V-11B-cot) [Code ğŸ’»](https://github.com/PKU-YuanGroup/LLaVA-CoT)


## Open-Source Project

* [Multimodal Open R1 ğŸ’»](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal)
* [R1-V ğŸ’»](https://github.com/Deep-Agent/R1-V)  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V)
* [VLM-R1 ğŸ’»](https://github.com/om-ai-lab/VLM-R1)  ![VLM-R1](https://img.shields.io/github/stars/om-ai-lab/VLM-R1) [Demo ğŸ¤—](https://huggingface.co/spaces/omlab/VLM-R1-Referral-Expression)
